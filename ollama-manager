#!/usr/bin/env python3
"""
SpotMan Ollama Manager
Enhanced Ollama instance management integrated with SpotMan framework.
Provides dedicated Ollama server management while leveraging SpotMan's infrastructure.
"""

import argparse
import os
import sys
import time
from pathlib import Path
from typing import Dict, List, Optional
import yaml
import importlib.util

# Import SpotMan's AWSInstanceManager for core functionality
try:
    from spotman_core import AWSInstanceManager
except ImportError as e:
    print(f"Error importing SpotMan core library: {e}")
    print("Please make sure spotman_core.py is in the same directory as ollama-manager")
    sys.exit(1)

# Import boto3 after spotman import to catch AWS-specific errors separately
try:
    import boto3
    from botocore.exceptions import ClientError
except ImportError as e:
    print(f"Error importing AWS libraries: {e}")
    print("Please install AWS dependencies: pip install boto3 botocore")
    sys.exit(1)


class OllamaManager:
    """Enhanced Ollama instance manager that integrates with SpotMan framework."""
    
    def __init__(self, region: str = None, profile: str = None):
        """Initialize the Ollama manager with SpotMan integration."""
        self.spotman = AWSInstanceManager(region=region, profile=profile)
        self.region = self.spotman.region
        self.ec2_client = self.spotman.ec2_client
    
    def create_ollama_instance(self, name: str = None, instance_type: str = None, 
                             spot: bool = True, profile: str = None) -> Optional[str]:
        """Create a new Ollama instance using SpotMan framework."""
        if not name:
            name = f"ollama-{int(time.time())}"
        
        print(f"Creating Ollama instance: {name}")
        
        # Determine which profile to use
        if profile:
            profile_name = profile
        elif instance_type:
            # Map instance types to appropriate profiles
            type_to_profile = {
                # Budget CPU instances
                'm5.large': 'ollama-cpu-budget',
                'm5.xlarge': 'ollama-cpu-budget', 
                't3.large': 'ollama-cpu-budget',
                't3.xlarge': 'ollama-cpu-budget',
                
                # Performance CPU instances  
                'c5.2xlarge': 'ollama-cpu-performance',
                'c5.4xlarge': 'ollama-cpu-performance',
                'c5.9xlarge': 'ollama-cpu-performance',
                'c5.12xlarge': 'ollama-cpu-performance',
                
                # Standard GPU instances
                'g5.xlarge': 'ollama-gpu-standard',
                'g4dn.xlarge': 'ollama-gpu-standard',
                'g4dn.2xlarge': 'ollama-gpu-standard',
                
                # Premium GPU instances
                'g5.2xlarge': 'ollama-gpu-premium',
                'g5.4xlarge': 'ollama-gpu-premium',
                'g5.8xlarge': 'ollama-gpu-premium',
                'p3.2xlarge': 'ollama-gpu-premium',
            }
            
            profile_name = type_to_profile.get(instance_type, 'ollama-gpu-standard')
            print(f"Using profile '{profile_name}' for instance type '{instance_type}'")
        else:
            # Default to standard GPU profile
            profile_name = "ollama-gpu-standard"
        
        # Override instance type if specified and different from profile
        if instance_type:
            # Create a temporary profile with the custom instance type
            try:
                profile_config = self.spotman.load_profile(profile_name)
                profile_config['instance_type'] = instance_type
                
                # Save temporary profile
                temp_profile_path = f"/tmp/ollama-custom-{int(time.time())}.yaml"
                with open(temp_profile_path, 'w') as f:
                    yaml.dump(profile_config, f)
                
                print(f"Using custom instance type: {instance_type}")
                
                # Load from temporary profile
                temp_profile_name = f"ollama-custom-{int(time.time())}"
                
                # Copy to profiles directory temporarily
                script_dir = os.path.dirname(os.path.abspath(__file__))
                temp_dest = os.path.join(script_dir, 'profiles', f'{temp_profile_name}.yaml')
                os.makedirs(os.path.dirname(temp_dest), exist_ok=True)
                
                import shutil
                shutil.copy2(temp_profile_path, temp_dest)
                
                try:
                    instance_id = self.spotman.create_instance(temp_profile_name, name, "ollama")
                    return instance_id
                finally:
                    # Clean up temporary profile
                    try:
                        os.remove(temp_dest)
                        os.remove(temp_profile_path)
                    except:
                        pass
            except Exception as e:
                print(f"Error with custom instance type, using default profile: {e}")
                return self.spotman.create_instance(profile_name, name, "ollama")
        else:
            # Use profile as-is
            return self.spotman.create_instance(profile_name, name, "ollama")
    
    def list_ollama_profiles(self) -> None:
        """List available Ollama profiles with descriptions."""
        profiles_info = {
            'ollama-cpu-budget': {
                'instance_type': 'm5.large',
                'description': 'Budget CPU instance for development and smaller models (7B-13B)',
                'cost': '~$15-30/month',
                'performance': '2-8 tokens/second',
                'best_for': 'Development, testing, 7B models'
            },
            'ollama-cpu-performance': {
                'instance_type': 'c5.4xlarge', 
                'description': 'High-performance CPU for larger models without GPU',
                'cost': '~$60-120/month',
                'performance': '5-20 tokens/second',
                'best_for': '33B+ models, CPU-only inference'
            },
            'ollama-gpu-standard': {
                'instance_type': 'g5.xlarge',
                'description': 'Standard GPU instance with NVIDIA A10G (24GB VRAM)',
                'cost': '~$100-200/month', 
                'performance': '15-50 tokens/second',
                'best_for': '7B-70B models, GPU acceleration'
            },
            'ollama-gpu-premium': {
                'instance_type': 'g5.2xlarge',
                'description': 'Premium GPU instance for maximum performance',
                'cost': '~$200-400/month',
                'performance': '30-100+ tokens/second', 
                'best_for': '70B+ models, production workloads'
            }
        }
        
        print("\nAvailable Ollama Profiles:\n")
        
        for profile_name, info in profiles_info.items():
            print(f"üìã {profile_name}")
            print(f"   Instance Type: {info['instance_type']}")
            print(f"   Description: {info['description']}")
            print(f"   Estimated Cost: {info['cost']} (spot pricing)")
            print(f"   Performance: {info['performance']}")
            print(f"   Best For: {info['best_for']}")
            print()
        
        print("Usage Examples:")
        print("  ./ollama-manager create --profile ollama-cpu-budget --name ollama-dev")
        print("  ./ollama-manager create --profile ollama-gpu-standard --name ollama-prod")
        print("  ./ollama-manager create --type g5.2xlarge --name ollama-premium")
        print()

    def list_ollama_instances(self) -> List[Dict]:
        """List all Ollama instances."""
        return self.spotman.list_instances(app_class="ollama")
    
    def get_ollama_instance_status(self, instance_identifier: str) -> Optional[Dict]:
        """Get detailed status of an Ollama instance."""
        instance_id = self.spotman._resolve_instance_identifier(instance_identifier)
        if not instance_id:
            return None
        
        instances = self.list_ollama_instances()
        for instance in instances:
            if instance['InstanceId'] == instance_id:
                return instance
        
        print(f"Instance {instance_identifier} is not an Ollama instance")
        return None
    
    def start_ollama_instance(self, instance_identifier: str) -> bool:
        """Start/resume an Ollama instance."""
        return self.spotman.resume_hibernated_instance(instance_identifier)
    
    def stop_ollama_instance(self, instance_identifier: str, hibernate: bool = True) -> bool:
        """Stop/hibernate an Ollama instance."""
        if hibernate:
            return self.spotman.hibernate_instance(instance_identifier)
        else:
            return self.spotman.stop_instance(instance_identifier)
    
    def terminate_ollama_instance(self, instance_identifier: str) -> bool:
        """Terminate an Ollama instance."""
        return self.spotman.terminate_instance(instance_identifier)
    
    def check_ollama_hibernation_status(self, instance_identifier: str) -> None:
        """Check hibernation status of an Ollama instance."""
        self.spotman.check_hibernation_status(instance_identifier)
    
    def update_ollama_ssh_config(self, instance_identifier: str = None) -> bool:
        """Update SSH config for Ollama instances (includes port forwarding)."""
        if instance_identifier:
            instance_id = self.spotman._resolve_instance_identifier(instance_identifier)
            if not instance_id:
                return False
            return self.spotman.update_ssh_config(instance_id=instance_id)
        else:
            return self.spotman.update_ssh_config(app_class="ollama")
    
    def connect_to_ollama(self, instance_identifier: str, port: int = 11434) -> None:
        """Connect to Ollama instance via SSH with port forwarding."""
        self.spotman.connect_to_instance(instance_identifier, [port], "Ollama")
    
    def test_ollama_connection(self, instance_identifier: str, local_port: int = 11434) -> bool:
        """Test if Ollama service is accessible via port forwarding."""
        return self.spotman.test_service_connection(
            instance_identifier, local_port, "/api/version", "Ollama"
        )
    
    def show_ollama_logs(self, instance_identifier: str) -> None:
        """Show Ollama service logs from the instance."""
        self.spotman.show_service_logs(instance_identifier, "ollama")


def format_ollama_instances_table(instances: List[Dict]) -> None:
    """Format and print Ollama instances in a table."""
    # Use the generic table formatter from spotman_core with Ollama-specific parameters
    from spotman_core import AWSInstanceManager
    manager = AWSInstanceManager()  # Just for accessing the method
    manager.format_instances_table(instances, "Ollama LLM", [11434], "Ollama")


def main():
    """Main entry point for SpotMan Ollama Manager."""
    parser = argparse.ArgumentParser(
        description='SpotMan Ollama Manager - Enhanced Ollama instance management',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  %(prog)s list-profiles                             # Show available instance profiles
  %(prog)s create --name ollama01                    # Create with default GPU profile
  %(prog)s create --profile ollama-cpu-budget       # Create budget CPU instance
  %(prog)s create --profile ollama-gpu-premium      # Create premium GPU instance  
  %(prog)s create --name ollama-gpu --type g5.xlarge # Create with specific instance type
  %(prog)s list                                      # List all Ollama instances
  %(prog)s start ollama01                           # Start/resume Ollama instance
  %(prog)s stop ollama01                            # Stop Ollama instance
  %(prog)s status ollama01                          # Show instance status
  %(prog)s connect ollama01                         # SSH connect with port forwarding
  %(prog)s test ollama01                            # Test Ollama API connection
  %(prog)s logs ollama01                            # Show Ollama service logs
  %(prog)s update-ssh                               # Update SSH config for all Ollama instances
  %(prog)s terminate ollama01                       # Terminate instance
        """
    )
    
    parser.add_argument('--region', help='AWS region')
    parser.add_argument('--aws-profile', help='AWS profile to use')
    
    subparsers = parser.add_subparsers(dest='command', help='Available commands')
    
    # Create command
    create_parser = subparsers.add_parser('create', help='Create a new Ollama instance')
    create_parser.add_argument('--name', help='Instance name (auto-generated if not specified)')
    create_parser.add_argument('--type', help='Instance type (e.g., g5.xlarge, m5.large)')
    create_parser.add_argument('--profile', help='Profile to use (ollama-gpu-standard, ollama-cpu-budget, etc.)')
    create_parser.add_argument('--no-spot', action='store_true', help='Use on-demand instead of spot')
    
    # List command
    list_parser = subparsers.add_parser('list', help='List all Ollama instances')
    
    # List profiles command
    profiles_parser = subparsers.add_parser('list-profiles', help='List available Ollama profiles')
    
    # Start command
    start_parser = subparsers.add_parser('start', help='Start/resume an Ollama instance')
    start_parser.add_argument('instance', help='Instance name or ID')
    
    # Stop command
    stop_parser = subparsers.add_parser('stop', help='Stop/hibernate an Ollama instance')
    stop_parser.add_argument('instance', help='Instance name or ID')
    stop_parser.add_argument('--no-hibernate', action='store_true', help='Stop without hibernation')
    
    # Status command
    status_parser = subparsers.add_parser('status', help='Show Ollama instance status')
    status_parser.add_argument('instance', help='Instance name or ID')
    
    # Connect command
    connect_parser = subparsers.add_parser('connect', help='SSH connect with port forwarding')
    connect_parser.add_argument('instance', help='Instance name or ID')
    connect_parser.add_argument('--port', type=int, default=11434, help='Local port for forwarding (default: 11434)')
    
    # Test command
    test_parser = subparsers.add_parser('test', help='Test Ollama API connection')
    test_parser.add_argument('instance', help='Instance name or ID')
    test_parser.add_argument('--port', type=int, default=11434, help='Local port to test (default: 11434)')
    
    # Logs command
    logs_parser = subparsers.add_parser('logs', help='Show Ollama service logs')
    logs_parser.add_argument('instance', help='Instance name or ID')
    
    # Update SSH command
    ssh_parser = subparsers.add_parser('update-ssh', help='Update SSH config for Ollama instances')
    ssh_parser.add_argument('instance', nargs='?', help='Specific instance to update (optional)')
    
    # Terminate command
    terminate_parser = subparsers.add_parser('terminate', help='Terminate an Ollama instance')
    terminate_parser.add_argument('instance', help='Instance name or ID')
    
    args = parser.parse_args()
    
    if not args.command:
        parser.print_help()
        sys.exit(1)
    
    # Initialize manager
    manager = OllamaManager(region=args.region, profile=args.aws_profile)
    
    # Execute commands
    if args.command == 'create':
        manager.create_ollama_instance(
            name=args.name, 
            instance_type=args.type,
            spot=not args.no_spot,
            profile=args.profile
        )
    
    elif args.command == 'list':
        instances = manager.list_ollama_instances()
        format_ollama_instances_table(instances)
    
    elif args.command == 'list-profiles':
        manager.list_ollama_profiles()
    
    elif args.command == 'start':
        manager.start_ollama_instance(args.instance)
    
    elif args.command == 'stop':
        manager.stop_ollama_instance(args.instance, hibernate=not args.no_hibernate)
    
    elif args.command == 'status':
        instance = manager.get_ollama_instance_status(args.instance)
        if instance:
            print(f"\nOllama Instance Status:")
            print(f"  Name: {instance['Name']}")
            print(f"  ID: {instance['InstanceId']}")
            print(f"  Type: {instance['InstanceType']}")
            print(f"  State: {instance['State']}")
            print(f"  Public IP: {instance['PublicIpAddress']}")
            print(f"  Launch Time: {instance['LaunchTime']}")
            
            if instance['State'] == 'running':
                print(f"\nüì° Connection: ssh {instance['Name'] if instance['Name'] != 'N/A' else instance['InstanceId']}")
                print(f"üåê Ollama API: http://localhost:11434 (when connected)")
            
            print()
            manager.check_ollama_hibernation_status(args.instance)
    
    elif args.command == 'connect':
        manager.connect_to_ollama(args.instance, args.port)
    
    elif args.command == 'test':
        manager.test_ollama_connection(args.instance, args.port)
    
    elif args.command == 'logs':
        manager.show_ollama_logs(args.instance)
    
    elif args.command == 'update-ssh':
        manager.update_ollama_ssh_config(args.instance)
    
    elif args.command == 'terminate':
        manager.terminate_ollama_instance(args.instance)


if __name__ == '__main__':
    main()