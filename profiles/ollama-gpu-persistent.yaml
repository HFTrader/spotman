# Ollama LLM Server on Persistent GPU Instance Profile
# Optimized for GPU-accelerated LLM workloads with persistent spot pricing
# No hibernation - just persistent spot instances that won't be terminated by AWS

# Instance configuration - GPU optimized for LLM performance
instance_type: "g5.xlarge"  # NVIDIA A10G GPU with 24GB VRAM
spot_instance: true  # Cost-effective spot pricing
spot_price: 0.50  # Higher bid to ensure persistence (on-demand is ~$1.00/hr)
hibernation_enabled: false  # No hibernation needed - just persistent

# Storage configuration - adequate for models
root_volume_size: 100  # GB - space for models and system
root_volume_type: "gp3"  # Fast storage for model loading
root_volume_encrypted: false  # No encryption needed without hibernation

# Operating system configuration
os_type: "ubuntu"
ami_id: "ami-0ea3c35c5c3284d82"  # Ubuntu 24.04 LTS (Noble) in us-east-2
update_os: true  # Keep system current

# SSH key configuration
key_name: "amz-ohio"  # SSH key for us-east-2 region

# SSH port forwarding configuration
ssh_port_forwards:
  - local_port: 11434     # Local port to forward
    remote_port: 11434    # Remote port (Ollama API)
    remote_host: localhost

# Ollama-specific setup script
user_data: !include ../scripts/ollama-setup.sh

# Tags for Ollama LLM server instances
tags:
  ApplicationClass: "ollama"
  Environment: "development" 
  InstanceType: "gpu-persistent"
  HibernationEnabled: "false"
  Purpose: "GPU-LLM-server"
  ManagedBy: "spotman"
  Service: "ollama"
  
# Note: SSH configuration will include port forwarding for Ollama
# Default Ollama port: 11434
# Connect with: ssh ollama-<instance-name>
# The setup script will install and configure Ollama automatically with GPU support

# Usage:
#   ./ollama-manager create --profile ollama-gpu-persistent --name ollama-gpu01
#   ./spotman create --profile ollama-gpu-persistent --name ollama-gpu01 --class ollama
#   
# Benefits:
#   - NVIDIA A10G GPU with 24GB VRAM for fast inference
#   - Persistent spot instance (won't be terminated by AWS spot interruptions)
#   - ~50% cost savings vs on-demand pricing
#   - No hibernation complexity - just persistent running
#   - GPU-accelerated Ollama for much faster model performance