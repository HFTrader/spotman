# Ollama LLM Server on Spot Instance Profile
# Optimized for running Ollama with hibernation support and cost-effective spot pricing
# Integrated from aws_ollama_manager.py into SpotMan framework

# Instance configuration - optimized for GPU-accelerated LLM workloads
instance_type: "g5.xlarge"  # GPU instance for better LLM performance
spot_instance: true  # Cost-effective spot pricing
hibernation_enabled: true  # Preserve state during spot interruptions

# Storage configuration - adequate for models and hibernation
root_volume_size: 100  # GB - space for models and hibernation image
root_volume_type: "gp3"  # Fast storage for model loading

# Operating system configuration
os_type: "ubuntu"
update_os: true  # Keep system current

# SSH port forwarding configuration
ssh_port_forwards:
  - local_port: 11434     # Local port to forward
    remote_port: 11434    # Remote port (defaults to local_port if not specified)
    remote_host: localhost # Remote host (defaults to localhost if not specified)

# Ollama-specific setup script
user_data: !include scripts/ollama-setup.sh

# Tags for Ollama LLM server instances
tags:
  ApplicationClass: "ollama"
  Environment: "development" 
  InstanceType: "spot"
  HibernationEnabled: "true"
  Purpose: "LLM-server"
  ManagedBy: "spotman"
  Service: "ollama"
  
# Note: SSH configuration will include port forwarding for Ollama
# Default Ollama port: 11434
# Connect with: ssh ollama-<instance-name>
# The setup script will install and configure Ollama automatically

# Usage:
#   ./spotman create --profile ollama-spot --name ollama01 --class ollama
#   ./spotman hibernate start ollama01  # Hibernate to save costs
#   ./spotman hibernate resume ollama01  # Resume when needed