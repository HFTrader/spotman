# Ollama LLM Server on GPU Spot Instance Profile
# Optimized for GPU-accelerated Ollama with persistent spot pricing
# High-performance NVIDIA A10G GPU for fast LLM inference

# Instance configuration - GPU optimized for LLM performance
instance_type: "g5.xlarge"  # NVIDIA A10G GPU with 24GB VRAM
spot_instance: true  # Cost-effective spot pricing
spot_price: 0.50  # Higher bid to ensure persistence (on-demand is ~$1.00/hr)
hibernation_enabled: false  # No hibernation - just persistent spot instances
key_name: "amz-ohio"  # SSH key pair for connecting to the instance

# Storage configuration - adequate for models
root_volume_size: 100  # GB - space for models and system
root_volume_type: "gp3"  # Fast storage for model loading
root_volume_encrypted: false  # No encryption needed without hibernation

# Operating system configuration
os_type: "ubuntu"
ami_id: "ami-0ea3c35c5c3284d82"  # Ubuntu 24.04 LTS (Noble) in us-east-2
update_os: true  # Keep system current

# SSH port forwarding configuration
ssh_port_forwards:
  - local_port: 11434     # Local port to forward
    remote_port: 11434    # Remote port (defaults to local_port if not specified)
    remote_host: localhost # Remote host (defaults to localhost if not specified)

# Ollama-specific setup script
user_data: !include ../scripts/ollama-setup.sh

# Tags for Ollama LLM server instances
tags:
  ApplicationClass: "ollama"
  Environment: "development" 
  InstanceType: "gpu-persistent"
  HibernationEnabled: "false"
  Purpose: "LLM-server"
  ManagedBy: "spotman"
  Service: "ollama"
  
# Note: SSH configuration will include port forwarding for Ollama
# Default Ollama port: 11434
# Connect with: ssh ollama-<instance-name>
# The setup script will install and configure Ollama automatically

# Usage:
#   ./ollama-manager create --name ollama-gpu01
#   ./ollama-manager create --name ollama-gpu01 --type g5.xlarge  # Default type
#   ./ollama-manager create --name ollama-gpu01 --no-spot        # On-demand pricing